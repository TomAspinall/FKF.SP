---
title: "Fast Kalman Filtering using Sequential Processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fast Kalman Filtering using Sequential Processing}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

This document provides worked examples of Kalman filtering using the 'fkf.SP' function of the 'FKF.SP' package. The 'fkf' function of the package 'FKF' (Fast Kalman Filter) is a well-established function call of the Kalman filter algorithm that is designed to maximize computational efficiency of the filtering process. The 'fkf.SP' function builds from the 'fkf' function by taking the additional assumption that the variance of disturbances of the measurement equation are independent. This allows filtering to be performed through a sequential processing method (i.e. a univariate treatment of the multivariate process) - increasing computational efficiency in the general case. This vignette provides four worked examples, comparing the computational efficiencies of the 'fkf' and 'fkf.SP' functions for maximum likelihood estimation (MLE). The first three examples were first presented within the associated vignette of the 'FKF' package, with the fourth being unique to the vignette. As well as the increase in processing time generated by the 'fkf.SP' function, this vignette further presents and explains the difference in log-likelihood values returned by the 'fkf' and 'fkf.SP' functions when there are missing observations (i.e. NA's are present within argument 'yt').

```{r setup}
library(FKF.SP)
##The package 'FKF' is required for this Vignette:
# install.packages("FKF")
library(FKF)
```

## Example 1 - ARMA(2,1) model estimation.

Autoregression moving average models can be estimated through Kalman filtering. See also help(makeARIMA) and help(KalmanRun).

Step 1 - Sample from an ARMA(2, 1) process through the 'stats' package to generate observations:

```{r}
# Set constants:
## Length of series
n <- 10000

## AR parameters
AR <- c(ar1 = 0.6, ar2 = 0.2, ma1 = -0.2, sigma = sqrt(0.2))

# Generate observations:
set.seed(1)
a <- stats::arima.sim(model = list(ar = AR[c("ar1", "ar2")], ma = AR["ma1"]), n = n,
            innov = rnorm(n) * AR["sigma"])
```

Step 2 - Create a state space representation of the four ARMA parameters:

```{r}
arma21ss <- function(ar1, ar2, ma1, sigma) {
Tt <- matrix(c(ar1, ar2, 1, 0), ncol = 2)
Zt <- matrix(c(1, 0), ncol = 2)
ct <- matrix(0)
dt <- matrix(0, nrow = 2)
GGt <- matrix(0)
H <- matrix(c(1, ma1), nrow = 2) * sigma
HHt <- H %*% t(H)
a0 <- c(0, 0)
## Diffuse assumption
P0 <- matrix(1e6, nrow = 2, ncol = 2)
return(list(a0 = a0, P0 = P0, ct = ct, dt = dt, Zt = Zt, Tt = Tt, GGt = GGt,
            HHt = HHt))}
```

Parameter estimation is performed through MLE, which involves optimizing the log-likelihood returned by the Kalman filter through the 'optim' function.

```{r}
# The objective function passed to 'optim'
objective <- function(theta, yt, SP) {
param <- arma21ss(theta["ar1"], theta["ar2"], theta["ma1"], theta["sigma"])
# Kalman filtering through the 'fkf.SP' function:
if(SP){
 ans <- - fkf.SP(a0 = param$a0, P0 = param$P0, dt = param$dt, ct = param$ct, 
               Tt = param$Tt, Zt = param$Zt, HHt = param$HHt, GGt = param$GGt, 
               yt = yt)
 }
# Kalman filtering through the 'fkf' function:
 else{
 ans <- - fkf(a0 = param$a0, P0 = param$P0, dt = param$dt, ct = param$ct, Tt = param$Tt,
            Zt = param$Zt, HHt = param$HHt, GGt = param$GGt, yt = yt)$logLik
   
 }
 return(ans)
}
##Optim minimizes functions by default, so the negative is returned

```

Step 3 - Estimate parameters through MLE:

```{r}
#This test estimates parameters through 'optim'.
#Please run the complete chunk for a fair comparison:

#Initial values:
theta <- c(ar = c(0, 0), ma1 = 0, sigma = 1)

###MLE through the 'fkf' function:
start <- Sys.time()
set.seed(1)
FKF_estimation <- optim(theta, objective, yt = rbind(a), hessian = TRUE, SP = F)
FKF_runtime <- Sys.time() - start

###MLE through the 'fkf.SP' function:
start <- Sys.time()
set.seed(1)
FKF.SP_estimation <- optim(theta, objective, yt = rbind(a), hessian = TRUE, SP = T)
FKF.SP_runtime <- Sys.time() - start

```

The MLE process applying both functions has returned identical estimated parameters:

```{r}
print(rbind(FKF.SP = FKF.SP_estimation$par, FKF = FKF_estimation$par))
```

As well as an identical call count number for both functions:

```{r}
print(c(FKF.SP = FKF.SP_estimation$counts[1], FKF = FKF_estimation$counts[1]))
```

Utilizing Sequential Processing however, we've decreased processing time:

```{r}
print(c(FKF.SP = FKF.SP_runtime, FKF = FKF_runtime))
```

The vignette of 'FKF' shows how to filter the series with estimated parameter values and develop some plots for analysis purposes. 'fkf.SP' is only appropriate for efficient parameter estimation, rather than the filtering under estimated parameters.

## Example 2 - Local level model for the Nile's annual flow:

This example presents differences in the computational time of the 'fkf.SP' and 'fkf' functions to the Nile dataset. It also shows the difference in log-likelihood values returned by the two functions that occurs when NAs are within observations.

```{r}
## Transition equation:
## alpha[t+1] = alpha[t] + eta[t], eta[t] ~ N(0, HHt)
## Measurement equation:
## y[t] = alpha[t] + eps[t], eps[t] ~  N(0, GGt)

##Complete Nile Data - no NA's
y_complete <- y_incomplete <- Nile
##Incomplete Nile Data - two NA's are present:
y_incomplete[c(3, 10)] <- NA


## Set constant parameters:
dt <- ct <- matrix(0)
Zt <- Tt <- matrix(1)
a0 <- y_incomplete[1]   # Estimation of the first year flow
P0 <- matrix(100)     # Variance of 'a0'

## Parameter estimation - maximum likelihood estimation:
Nile_MLE <- function(yt, SP){
##Unknown parameters initial estimates:
GGt <- HHt <- var(yt, na.rm = TRUE) * .5
set.seed(1)
# Kalman filtering through the 'fkf.SP' function:
if(SP){
  return(optim(c(HHt = HHt, GGt = GGt),
        fn = function(par, ...)
             -fkf.SP(HHt = matrix(par[1]), GGt = matrix(par[2]), ...),
             yt = rbind(yt), a0 = a0, P0 = P0, dt = dt, ct = ct,
             Zt = Zt, Tt = Tt))
} else {
# Kalman filtering through the 'fkf' function:
  return(optim(c(HHt = HHt, GGt = GGt),
        fn = function(par, ...)
             -fkf(HHt = matrix(par[1]), GGt = matrix(par[2]), ...)$logLik,
             yt = rbind(yt), a0 = a0, P0 = P0, dt = dt, ct = ct,
             Zt = Zt, Tt = Tt))
}}
```

Performing parameter estimation using complete data, the fkf and fkf.SP functions return identical results:

```{r}
fkf.SP_MLE_complete <- Nile_MLE(y_complete, SP = T)
fkf_MLE_complete <- Nile_MLE(y_complete, SP = F)
```

fkf.SP:
```{r}
print(fkf.SP_MLE_complete[1:3])
```

fkf:
```{r}
print(fkf_MLE_complete[1:3])
```

Performing parameter estimation using incomplete data returns identical estimated parameters, but different log-likelihood values:

```{r}
fkf.SP_MLE_incomplete <- Nile_MLE(y_incomplete, SP = T)
fkf_MLE_incomplete <- Nile_MLE(y_incomplete, SP = F)
```

'fkf.SP':
```{r}
print(fkf.SP_MLE_incomplete[1:3])
```

'fkf':
```{r}
print(fkf_MLE_incomplete[1:3])
```

The difference in log-likelihood values is equal to `r fkf_MLE_incomplete$value - fkf.SP_MLE_incomplete$value`. This difference is equal to:

```{r}
#Number of NA values:
NA_values <- length(which(is.na(y_incomplete)))

print( 0.5 * NA_values * log(2 * pi))
```

The log-likelihood score for the Kalman filter is given by:

$$ - \frac{1}{2}(n \times d \times log(2\pi)) - \frac{1}{2}\sum_{t=1}^{n}(log|F_t| + v'F^{-1}v)$$
where $n$ is the number of discrete time-steps (i.e. the number of columns of object 'yt') and $d$ is the number of observations at each time point (i.e. the number of rows of object 'yt'). $v$ and $F_t$ are the measurement error and function of the covariance matrix at time $t$ respectively. The 'fkf' function instantiates its log-likelihood score by calculating $- 0.5 \times n \times d \times log(2\pi)$. Under the scenario where there are missing observations, however, $d$ would instead become $d_t$ where $d_t \leq d \forall t$. The instantiated log-likelihood term would instead be $- 0.5 ((n \times d)-2) \times log(2\pi)$, explaining this difference in log-likelihood scores. The 'fkf' function therefore instantiates the log-likelihood score of two observations that are not actually observed.

### Speed Comparison - Nile Data (10,000 iterations):
```{r}
#This test uses estimated parameters of complete data. 
#Please run the complete chunk for a fair comparison:

#'fkf' 
set.seed(1)
start <- Sys.time()
for(i in 1:1e4) fkf(a0, P0, dt, ct, Tt, Zt, HHt = matrix(fkf_MLE_complete$par[1]),
                    GGt = matrix(fkf_MLE_complete$par[2]), yt = rbind(y_complete))
FKF_runtime <- Sys.time() - start

#FKF.SP
set.seed(1)
start = Sys.time()
for(i in 1:1e4) fkf.SP(a0, P0, dt, ct, Tt, Zt, HHt = matrix(fkf.SP_MLE_complete$par[1]),
                       GGt = matrix(fkf.SP_MLE_complete$par[2]), yt = rbind(y_complete))
fkf.SP_runtime <- Sys.time() - start

print(c(FKF.SP = fkf.SP_runtime, FKF = FKF_runtime))

```

Utilizing Sequential Processing has decreased processing time.

## Example 3 - Tree Ring Data:

```{r}

## Transition equation:
## alpha[t+1] = alpha[t] + eta[t], eta[t] ~ N(0, HHt)
## Measurement equation:
## y[t] = alpha[t] + eps[t], eps[t] ~  N(0, GGt)

## tree-ring widths in dimensionless units
y <- treering

## Set constant parameters:
dt <- ct <- matrix(0)
Zt <- Tt <- matrix(1)
a0 <- y[1]            # Estimation of the first width
P0 <- matrix(100)     # Variance of 'a0'

##Time comparison - Estimate parameters 10 times:

###MLE through the 'fkf' function:
start = Sys.time()
set.seed(1)
for(i in 1:10)  fit.fkf <- optim(c(HHt = var(y, na.rm = TRUE) * .5,
                     GGt = var(y, na.rm = TRUE) * .5),
                   fn = function(par, ...)
                     -fkf(HHt = array(par[1],c(1,1,1)), GGt = array(par[2],c(1,1,1)), ...)$logLik,
                   yt = rbind(y), a0 = a0, P0 = P0, dt = dt, ct = ct,
                   Zt = Zt, Tt = Tt)

run.time_FKF = Sys.time() - start

###MLE through the 'fkf.SP' function:
start = Sys.time()
set.seed(1)
for(i in 1:10)  fit.fkf.SP <- optim(c(HHt = var(y, na.rm = TRUE) * .5,
                        GGt = var(y, na.rm = TRUE) * .5),
                      fn = function(par, ...)
                        -fkf.SP(HHt = array(par[1],c(1,1,1)), GGt = matrix(par[2]), ...),
                      yt = rbind(y), a0 = a0, P0 = P0, dt = dt, ct = ct,
                      Zt = Zt, Tt = Tt)
run.time_FKF.SP = Sys.time() - start

print(c(fkf.SP = run.time_FKF.SP, fkf = run.time_FKF))

## Filter tree ring data with estimated parameters using 'fkf':
fkf.obj <- fkf(a0, P0, dt, ct, Tt, Zt, HHt = array(fit.fkf$par[1],c(1,1,1)),
               GGt = array(fit.fkf$par[2],c(1,1,1)), yt = rbind(y))

```

Utilizing Sequential Processing has decreased processing time.

## Example 4 - Fitting a Geometric Brownian Motion (GBM) to Term Structure Data:

The Kalman filter can be used to estimate models that forecast the prices of a commodity using time-series data of quoted prices of futures contracts of that commodity. The following example estimates the parameters of a random walk (i.e. GBM) model for crude oil through MLE. Quoted futures contracts are available in the 'TSCPM' package. See the 'TSCPM' documentation for more details on fitting commodity pricing models to term structure data.

Step 1 - load in the 'TSCPM' package

```{r}
library(TSE)
```

Step 2 - develop the objective function:

```{r}

yt = t(log(TSE::SS.Oil$Contracts)) # quoted log futures prices
delta.t <- TSE::SS.Oil$dt # Discrete time step
##time to maturity of quoted futures contracts:
TTM <- t(TSE::SS.Oil$Contract.Maturities)

a0 <- yt[1,1]     # initial estimate
P0 <- matrix(100) # Variance of 'a0'

## GBM Function
gbm.mle <- function(theta, SP){

ct <- theta["alpha.rn"] * TTM
dt <- (theta["alpha"] - 0.5 * theta["sigma"]^2) * delta.t
Zt <- matrix(1, nrow(yt))
HHt <- matrix(theta["sigma"]^2 * delta.t)
Tt <- matrix(1)

##'fkf.SP' requires a vector of the diagonal elements of the variances of the measurement error 
if(SP){
GGt = rep(theta["sigma.epsilon"]^2, nrow(yt))
} else {
##'fkf' instead requires a matrix of the elements of the variances of the measurement error 
GGt = diag(theta["sigma.epsilon"]^2, nrow(yt))
}

logLik = ifelse(SP,
                - fkf.SP(a0 = a0, P0 = P0, dt = dt, ct = ct, Tt = Tt, Zt = Zt, HHt = HHt, GGt = GGt, yt = yt),
                - fkf(a0 = a0, P0 = P0, dt = dt, ct = ct, Tt = Tt, Zt = Zt, HHt = HHt, GGt = GGt, yt = yt)$logLik
                )
return(logLik)
}

```

Step 3 - Perform MLE:

```{r}

#Initial estimates
gbm.par <- c(alpha = 0, alpha.rn = 0.01, sigma = 0.1, sigma.epsilon = 0.05)

###MLE through the 'fkf.SP' function:
set.seed(1)
start = Sys.time()
fkf.SP.gbm = optim(par = gbm.par, fn = gbm.mle, SP = T)
fkf.SP_runtime <- Sys.time() - start

###MLE through the 'fkf' function:
set.seed(1)
start = Sys.time()
fkf.gbm = optim(par = gbm.par, fn = gbm.mle, SP = F)
fkf_runtime <- Sys.time() - start

```

The presence of a large number of NA's in the observation matrix (i.e. object 'yt') has resulted in significantly different MLE scores of both functions (see Example 3 for more details):

```{r}
print(rbind(FKF.SP = fkf.SP.gbm$value, FKF = fkf.gbm$value))
```

Regardless, The MLE process applying both functions has returned nearly identical estimated parameters:

```{r}
print(rbind(FKF.SP = fkf.SP.gbm$par, FKF = fkf.gbm$par))
```

As well as a nearly identical call count number for both functions:

```{r}
print(c(FKF.SP = fkf.SP.gbm$counts[1], FKF = fkf.gbm$counts[1]))
```

A sequential processing approach, however, has significantly decreased processing time:

```{r}
print(c(FKF.SP = fkf.SP_runtime, FKF = fkf_runtime))
```

Sequential processing is a significantly faster Kalman filtering approach for this particular example due to the large number of observations at each time point, the assumption that the variance of the disturbances are independent, the large number of NA's that are observed as contracts expired or are made available and the dimensionality of argument 'GGt' being significantly reduced.

